{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# BPE Tokenizer from First Principles\n",
    "\n",
    "This notebook implements Byte-Pair Encoding (BPE) from scratch to understand the fundamental mechanics of subword tokenization used in modern language models.\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "1. **Initialize:** Start with character-level tokens for each word\n",
    "2. **Iterate:** Find the most frequent adjacent pair and merge them\n",
    "3. **Repeat:** Continue until reaching desired vocabulary size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        \"\"\"\n",
    "        Initialize BPE tokenizer with target vocabulary size.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Target size of final vocabulary\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = {}\n",
    "        self.vocab = set()\n",
    "        self.merges = []\n",
    "    \n",
    "    def _get_word_tokens(self, word):\n",
    "        \"\"\"\n",
    "        Convert a word into character-level tokens.\n",
    "        \n",
    "        Args:\n",
    "            word (str): Input word\n",
    "            \n",
    "        Returns:\n",
    "            list: List of character tokens\n",
    "        \"\"\"\n",
    "        # Shahab, implement this method\n",
    "        # Hint: Split word into individual characters, add </w> at the end\n",
    "        tokens=list(word)\n",
    "        tokens.append(\"</w>\")\n",
    "        return tokens\n",
    "    \n",
    "    def _get_stats(self):\n",
    "        \"\"\"\n",
    "        Count frequency of all adjacent pairs in the current vocabulary.\n",
    "        \n",
    "        Returns:\n",
    "            Counter: Frequency count of adjacent pairs\n",
    "        \"\"\"\n",
    "        # Shahab, implement this method\n",
    "        # Hint: Look at all adjacent pairs across all words and their frequencies\n",
    "        pairs_count = Counter()\n",
    "        for word,freq in self.word_freqs.items():\n",
    "            for i in range(len(word)-1):\n",
    "                pair = (word[i],word[i+1])\n",
    "                pairs_count[pair]+=freq\n",
    "        return pairs_count\n",
    "    \n",
    "    def _merge_vocab(self, pair):\n",
    "        \"\"\"\n",
    "        Merge the most frequent pair throughout the vocabulary.\n",
    "        \n",
    "        Args:\n",
    "            pair (tuple): The pair to merge (e.g., ('a', 't'))\n",
    "        \"\"\"\n",
    "        # Shahab, implement this method\n",
    "        # Hint: Replace all occurrences of the pair with the merged token\n",
    "        new_word_freqs={}\n",
    "        for word,freq in self.word_freqs.items():\n",
    "            new_word = []\n",
    "            i=0\n",
    "            while i < len(word):\n",
    "                if i < len(word)-1 and (word[i],word[i+1])==pair:\n",
    "                    new_word.append(pair[0]+pair[1])\n",
    "                    i=i+2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i=i+1\n",
    "            new_word_freqs[tuple(new_word)]=freq\n",
    "        self.word_freqs=new_word_freqs\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer on a corpus.\n",
    "        \n",
    "        Args:\n",
    "            corpus (list): List of sentences/documents\n",
    "        \"\"\"\n",
    "        # Step 1: Extract word frequencies from corpus\n",
    "        all_words = []\n",
    "        for text in corpus:\n",
    "            words = re.findall(r'\\w+', text.lower())\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        # Count word frequencies\n",
    "        word_counts = Counter(all_words)\n",
    "        \n",
    "        # Step 2: Initialize vocabulary with character-level tokens\n",
    "        for word, freq in word_counts.items():\n",
    "            self.word_freqs[tuple(self._get_word_tokens(word))] = freq\n",
    "        \n",
    "        # Build initial character vocabulary\n",
    "        for word_tokens in self.word_freqs.keys():\n",
    "            self.vocab.update(word_tokens)\n",
    "        \n",
    "        print(f\"Initial vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Target vocabulary size: {self.vocab_size}\")\n",
    "        \n",
    "        # Step 3: Iteratively merge most frequent pairs\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pairs = self._get_stats()\n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best_pair = pairs.most_common(1)[0][0]\n",
    "            print(f\"Merging: {best_pair}\")\n",
    "            \n",
    "            self._merge_vocab(best_pair)\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            # Add merged token to vocabulary\n",
    "            merged_token = best_pair[0] + best_pair[1]\n",
    "            self.vocab.add(merged_token)\n",
    "        \n",
    "        print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode text using the trained BPE model.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to encode\n",
    "            \n",
    "        Returns:\n",
    "            list: List of BPE tokens\n",
    "        \"\"\"\n",
    "        # Shahab, implement this method\n",
    "        # Hint: Apply the learned merges in order to tokenize new text\n",
    "        \n",
    "        words = re.findall(r'\\w+', text.lower())\n",
    "        all_tokens = []\n",
    "        for word in words:\n",
    "            word_tokens = self._get_word_tokens(word)\n",
    "            new_word_tokens = word_tokens\n",
    "            for pair in self.merges:\n",
    "                newer_word_tokens=[]\n",
    "                i=0\n",
    "                while i < len(new_word_tokens):\n",
    "                    if i < len(new_word_tokens)-1 and (new_word_tokens[i],new_word_tokens[i+1])==pair:\n",
    "                        newer_word_tokens.append(pair[0]+pair[1])\n",
    "                        i=i+2\n",
    "                    else:\n",
    "                        newer_word_tokens.append(new_word_tokens[i])\n",
    "                        i=i+1\n",
    "                new_word_tokens = newer_word_tokens\n",
    "            all_tokens.extend(new_word_tokens)\n",
    "        return all_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary size: 17\n",
      "Target vocabulary size: 50\n",
      "Merging: ('a', 't')\n",
      "Merging: ('e', '</w>')\n",
      "Merging: ('at', '</w>')\n",
      "Merging: ('t', 'h')\n",
      "Merging: ('th', 'e</w>')\n",
      "Merging: ('s', '</w>')\n",
      "Merging: ('a', 'n')\n",
      "Merging: ('at', 's</w>')\n",
      "Merging: ('c', 'at</w>')\n",
      "Merging: ('n', '</w>')\n",
      "Merging: ('r', 'at</w>')\n",
      "Merging: ('c', 'ats</w>')\n",
      "Merging: ('an', 'd')\n",
      "Merging: ('and', '</w>')\n",
      "Merging: ('r', 'ats</w>')\n",
      "Merging: ('an', 'i')\n",
      "Merging: ('ani', 'm')\n",
      "Merging: ('anim', 'a')\n",
      "Merging: ('anima', 'l')\n",
      "Merging: ('animal', 's</w>')\n",
      "Merging: ('s', 'at</w>')\n",
      "Merging: ('o', 'n</w>')\n",
      "Merging: ('m', 'at</w>')\n",
      "Merging: ('s', 'a')\n",
      "Merging: ('sa', 'w')\n",
      "Merging: ('saw', '</w>')\n",
      "Merging: ('r', 'an')\n",
      "Merging: ('ran', '</w>')\n",
      "Merging: ('f', 'r')\n",
      "Merging: ('fr', 'o')\n",
      "Merging: ('fro', 'm')\n",
      "Merging: ('from', '</w>')\n",
      "Merging: ('a', 'r')\n",
      "Final vocabulary size: 50\n",
      "\n",
      "Learned merges:\n",
      "1: a + t -> at\n",
      "2: e + </w> -> e</w>\n",
      "3: at + </w> -> at</w>\n",
      "4: t + h -> th\n",
      "5: th + e</w> -> the</w>\n",
      "6: s + </w> -> s</w>\n",
      "7: a + n -> an\n",
      "8: at + s</w> -> ats</w>\n",
      "9: c + at</w> -> cat</w>\n",
      "10: n + </w> -> n</w>\n",
      "11: r + at</w> -> rat</w>\n",
      "12: c + ats</w> -> cats</w>\n",
      "13: an + d -> and\n",
      "14: and + </w> -> and</w>\n",
      "15: r + ats</w> -> rats</w>\n",
      "16: an + i -> ani\n",
      "17: ani + m -> anim\n",
      "18: anim + a -> anima\n",
      "19: anima + l -> animal\n",
      "20: animal + s</w> -> animals</w>\n",
      "21: s + at</w> -> sat</w>\n",
      "22: o + n</w> -> on</w>\n",
      "23: m + at</w> -> mat</w>\n",
      "24: s + a -> sa\n",
      "25: sa + w -> saw\n",
      "26: saw + </w> -> saw</w>\n",
      "27: r + an -> ran\n",
      "28: ran + </w> -> ran</w>\n",
      "29: f + r -> fr\n",
      "30: fr + o -> fro\n",
      "31: fro + m -> from\n",
      "32: from + </w> -> from</w>\n",
      "33: a + r -> ar\n",
      "\n",
      "Final vocabulary: ['</w>', 'a', 'an', 'and', 'and</w>', 'ani', 'anim', 'anima', 'animal', 'animals</w>', 'ar', 'at', 'at</w>', 'ats</w>', 'c', 'cat</w>', 'cats</w>', 'd', 'e', 'e</w>', 'f', 'fr', 'fro', 'from', 'from</w>', 'h', 'i', 'l', 'm', 'mat</w>', 'n', 'n</w>', 'o', 'on</w>', 'r', 'ran', 'ran</w>', 'rat</w>', 'rats</w>', 's', 's</w>', 'sa', 'sat</w>', 'saw', 'saw</w>', 't', 'th', 'the</w>', 'u', 'w']\n",
      "\n",
      "Encoding 'the cat catches rats': ['the</w>', 'cat</w>', 'c', 'at', 'c', 'h', 'e', 's</w>', 'rats</w>']\n"
     ]
    }
   ],
   "source": [
    "# Test corpus - simple example\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the cat saw the rat\", \n",
    "    \"the rat ran from the cat\",\n",
    "    \"cats and rats are animals\",\n",
    "    \"animals run and cats catch rats\"\n",
    "]\n",
    "\n",
    "# Create and train tokenizer\n",
    "tokenizer = BPETokenizer(vocab_size=50)\n",
    "tokenizer.train(corpus)\n",
    "\n",
    "print(\"\\nLearned merges:\")\n",
    "for i, merge in enumerate(tokenizer.merges):\n",
    "    print(f\"{i+1}: {merge[0]} + {merge[1]} -> {merge[0] + merge[1]}\")\n",
    "\n",
    "print(f\"\\nFinal vocabulary: {sorted(tokenizer.vocab)}\")\n",
    "\n",
    "# Test encoding\n",
    "test_text = \"the cat catches rats\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "print(f\"\\nEncoding '{test_text}': {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
